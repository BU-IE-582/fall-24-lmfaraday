{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE 582 Homework 1 - FALL'24\n",
    "\n",
    "Berat Kubilay Güngez - 2021402087"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Related Literature](#RelatedLiterature)\n",
    "3. [Data Preprocessing and Analysis](#datainspection)\n",
    "    * 3a. [Importing Libraries](#datainspection)\n",
    "    * 3b. [Inspecting Output Data](#output)\n",
    "    * 3c. [Inspecting Input Data](#input)\n",
    "4. [Comparison in Linear Models](#linear)\n",
    "5. [Conclusions](#conclusion)\n",
    "6. [Code](#code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As high-frequency communication technologies like 5G evolve, designing efficient antennas has become crucial. Antenna performance, often evaluated by the S11 parameter, requires computationally intensive electromagnetic (EM) simulations, making traditional trial-and-error methods impractical. To address this, machine learning provides a data-driven approach to model and predict antenna characteristics based on design parameters. This assignment uses techniques like Principal Component Analysis (PCA) for dimensionality reduction and linear regression for predictive modeling to better understand and simplify the complex relationships within antenna design, aiming to improve efficiency in creating high-performance systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Literature <a name=\"RelatedLiterature\"></a>\n",
    "\n",
    "In electrical network analysis, S-parameters (scattering parameters) describe how an electromagnetic signal interacts at different network ports, typically in high-frequency circuits. The S11 parameter, a specific type of S-parameter, measures the reflection at the input port—indicating how much signal is reflected back rather than transmitted. This reflection coefficient helps engineers assess antenna efficiency and impedance matching, critical for minimizing signal loss in RF designs. S-parameters like S11 are widely used for their practicality in evaluating performance across frequencies without needing complex internal device details.\n",
    "\n",
    "For more, see [Ansys on S-parameters](https://www.ansys.com/simulation-topics/what-are-s-parameters#:~:text=Scattering%20parameters%20%E2%80%94%20also%20known%20as,stimulated%20by%20an%20electrical%20signal.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Analysis <a name=\"datainspection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Importing Libraries <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Inspecting Output Data <a name=\"output\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To comprehend the characteristics of S11 parameter data across various frequencies, the real and imaginary components of the data are imported. Then, the magnitude of the data is computed. Given that the magnitude of the S11 parameters encapsulates its performance, lower S11 parameter values indicate reduced signal reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ouput_data_loc = \"data/hw1_real.csv\" \n",
    "\n",
    "real_output_df = pd.read_csv(real_ouput_data_loc)\n",
    "\n",
    "img_output_data_loc = \"data/hw1_img.csv\" \n",
    "\n",
    "img_output_df = pd.read_csv(img_output_data_loc)\n",
    "\n",
    "output_df = np.sqrt(real_output_df **2 + img_output_df **2) # Magnitude of the S11 parameter\n",
    "\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, a single output is selected to analyze the distribution of the real part, imaginary part, and magnitude in comparison to one another. The third sample was chosen due to its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(output_df.iloc[2], label=\"Magnitude\")\n",
    "plt.plot(real_output_df.iloc[2], label=\"Real part\")\n",
    "plt.plot(img_output_df.iloc[2], label=\"İmaginary part\")\n",
    "\n",
    "plt.title(\"S11 Parameter of the 3rd Sample\")\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "\n",
    "plt.xticks(range(0, 200, 25))\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then first 15 samples are plotted to see different characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(0,15):\n",
    "    plt.plot(output_df.iloc[i], label=f\"output {i}\")\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=6)\n",
    "\n",
    "plt.title(\"Magnitude of the first 15 outputs\")\n",
    "\n",
    "plt.xticks(range(0, 200, 25))\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the plot above, each sample exhibits distinct characteristics at various frequency values. Among these samples, output 2 appears to perform best around the 60th frequency value compared to others. Approximately six outputs reflect nearly all of the signal back meaning that they do not perform well.\n",
    "\n",
    "To apply predictive approaches later, data needs to be simplified in a way that there will be only one output value that captures the variability of all frequencies. One option seems to be using minimum values. To make it more applicable, the average of the minimum of 25 values is calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values_output = output_df.apply(lambda x: x.nsmallest(25).mean(), axis=1)\n",
    "min_values_index = output_df.idxmin(axis=1)\n",
    "\n",
    "min_values_real = real_output_df.apply(lambda x: x.nsmallest(25).mean(), axis=1)\n",
    "min_values_img = img_output_df.apply(lambda x: x.nsmallest(25).mean(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below illustrates the output values that will be generated in the event of utilizing the average of the 25 minimum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(0,10):\n",
    "    plt.plot(output_df.iloc[i], label=f\"output {i}\")\n",
    "    plt.scatter(min_values_index[i], min_values_output[i], color=\"red\")\n",
    "\n",
    "plt.legend(loc=\"best\", fontsize=6)\n",
    "plt.title(\"Magnitued of output 0 to 9 with their avg. minimum values\")\n",
    "plt.xticks(range(0, 200, 25))\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other option is applying PCA to the output data. PCA is a dimensionality reduction technique that transforms data into a lower-dimensional space while preserving the variance of the data. By applying PCA to the output data, the number of output variables can be reduced to a smaller set of principal components that capture the most significant variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1) # Only one component is selected for simplicity\n",
    "\n",
    "principal_components = pca.fit_transform(output_df)\n",
    "\n",
    "explained_variance_df = pd.DataFrame({\n",
    "    'Standard deviation': np.sqrt(pca.explained_variance_),\n",
    "    'Proportion of Variance': pca.explained_variance_ratio_,\n",
    "}, index=[f'Comp.{i+1}' for i in range(len(pca.explained_variance_))])\n",
    "\n",
    "explained_variance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of Principal Component Analysis (PCA) indicate that 61% of the variability in the magnitude of S11 parameters can be captured by a single output. This finding is advantageous as variability is a crucial factor that is learned during the training process of Machine Learning models. Same technique is applied to the real and imaginary parts of the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1) # Only one component is selected for simplicity\n",
    "\n",
    "principal_components_real = pca.fit_transform(real_output_df)\n",
    "\n",
    "explained_variance_df = pd.DataFrame({\n",
    "    'Standard deviation': np.sqrt(pca.explained_variance_),\n",
    "    'Proportion of Variance': pca.explained_variance_ratio_,\n",
    "}, index=[f'Comp.{i+1}' for i in range(len(pca.explained_variance_))])\n",
    "\n",
    "explained_variance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA on the real part of the data is performed even better. Suggests a single value can capture 85% of the variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1) # Only one component is selected for simplicity\n",
    "\n",
    "principal_components_img = pca.fit_transform(img_output_df)\n",
    "\n",
    "explained_variance_df = pd.DataFrame({\n",
    "    'Standard deviation': np.sqrt(pca.explained_variance_),\n",
    "    'Proportion of Variance': pca.explained_variance_ratio_,\n",
    "}, index=[f'Comp.{i+1}' for i in range(len(pca.explained_variance_))])\n",
    "\n",
    "explained_variance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA on the imaginary part of the data performs worse compared to other data sets. This suggests that a single value can only capture 49% of the variability. However, this approach can still be effective in certain applications, depending on the specific use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Inspecting Input Data <a name=\"input\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_loc = \"data/hw1_input.csv\"\n",
    "\n",
    "input_df = pd.read_csv(input_data_loc)\n",
    "\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input values are composed of several geometric features. As can be observed in the accompanying table, these features have distinct ranges. Consequently, it is necessary to scale the data to prevent any potential effects. This scaling process is performed after an in-depth examination of the data as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.describe() # Descriptive statistics of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the relation between height of substrate and width of patch, there doesn’t seem to be a clear relationship between the input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.corr() # Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, besides the height of substrate and the width of patch, there doesn’t seem to be any interesting relation between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_plot = sns.pairplot(input_df, kind='scatter', diag_kind='kde', markers='o', plot_kws={'alpha':0.5}) # Pair plot of the input data\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see the relation between the height of substrate and the width of the patch more closely. As can be seen below, these two are scattered in two parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(input_df[\"height of substrate\"], input_df[\"width of patch\"])\n",
    "\n",
    "plt.title(\"Width of Patch vs Height of Substrate\")\n",
    "plt.xlabel(\"Height of Substrate\")\n",
    "plt.ylabel(\"Width of Patch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the dimensions of the space, variables can be combined as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df[\"width of patch combined with height of substrate\"] = np.where((input_df[\"width of patch\"] > 4) & (input_df[\"height of substrate\"] > 0.4), 1, 0)\n",
    "\n",
    "input_df.drop([\"width of patch\", \"height of substrate\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot below suggests that the combined relation of these two variables may help us to capture the dynamics of the minimum magnitude. Therefore, this manipulation will be used in the following models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = {0: \"red\", 1: \"blue\"} # Color mapping for the scatter plot to show the combined feature\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(range(len(min_values_index)), min_values_output, \n",
    "            c=input_df[\"width of patch combined with height of substrate\"].map(color)) \n",
    "\n",
    "plt.title(\"Minimum Magnitudes with respect to width of patch combined with height of substrate\")\n",
    "plt.ylabel(\"Minimum Magnitudes\")\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data needs to be scaled as previously discussed. To reduce the effects of outliers(if they exist), standardization is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "input_df_scaled = pd.DataFrame(scaler.fit_transform(input_df), columns=input_df.columns)\n",
    "\n",
    "input_df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation values between features and the average 25 minimum values of magnitude values are calculated below. It suggests the previously manipulated combined feature has a high correlation and may be effective in linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df_scaled.corrwith(min_values_output).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s apply the PCA method to further simplify the input data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = input_df_scaled.copy()\n",
    "\n",
    "pca_df.drop(\"width of patch combined with height of substrate\", axis=1, inplace=True) # Dropping the categorical feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical feature is dropped because one of the PCA's assumptions is that the data is numeric and is distributed normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA() # Do not limit the number of components to see the explained variance of all components\n",
    "\n",
    "principal_components = pca.fit(pca_df)\n",
    "\n",
    "explained_variance_df = pd.DataFrame({\n",
    "    'Standard deviation': np.sqrt(pca.explained_variance_),\n",
    "    'Proportion of Variance': pca.explained_variance_ratio_,\n",
    "    'Cumulative Proportion': np.cumsum(pca.explained_variance_ratio_)\n",
    "}, index=[f'Comp.{i+1}' for i in range(len(pca.explained_variance_))])\n",
    "\n",
    "explained_variance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table above suggests that there doesn’t seem to be a clear winner that captures most of the variability itself. Since most of the components have a similar proportion of the variance.\n",
    "\n",
    "See table below to understand and compare the content of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_df = pd.DataFrame(pca.components_.T, columns=[f'Comp.{i+1}' for i in range(len(pca.explained_variance_))], index=pca_df.columns)\n",
    "\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot below shows the variance explained by each component. 10% of the variance may be a good threshold to decide on the number of components to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = explained_variance_df[\"Proportion of Variance\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
    "plt.axhline(y=0.1, color='r', linestyle='--') # 10% explained variance threshold\n",
    "\n",
    "plt.title(\"Explained Variance of Principal Components\")\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.xticks(range(1, len(explained_variance) + 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot below shows the cumulative variance explained by the components. It suggests that around 80% of the variance can be explained by 7 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance = explained_variance_df[\"Cumulative Proportion\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "plt.axhline(y=0.8, color='r', linestyle='--')  # 80% cumulative explained variance threshold\n",
    "\n",
    "plt.title('Cumulative Explained Variance of Principal Components')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.xticks(range(1, len(cumulative_variance) + 1))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the number of cpomponents is decided as 7. Then the data is transformed into the new space with the addition of the categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_input_df = pd.DataFrame(pca.transform(pca_df), columns=[f'PC{i+1}' for i in range(len(pca.explained_variance_))])\n",
    "\n",
    "transformed_input_df.drop(columns=[\"PC9\", \"PC8\"], inplace=True, axis=1) # Dropping the last two components\n",
    "\n",
    "# Adding the categorical feature\n",
    "transformed_input_df[\"width of patch combined with height of substrate\"] = input_df_scaled[\"width of patch combined with height of substrate\"] \n",
    "\n",
    "transformed_input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison in Linear Models <a name=\"linear\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real part of the output is partitioned into training and testing sets to assess the performance of linear models. The training set is utilized for model training, while the testing set is employed to evaluate the model’s performance on unseen data. The training set comprises 80% of the data, and the testing set comprises the remaining 20%. It is important to note that a predetermined random state is employed to make comparison between the sets easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "\n",
    "X_train_transformed, X_test_transformed, y_train_min_real, y_test_min_real = train_test_split(transformed_input_df, min_values_real, test_size=0.2, random_state=5)\n",
    "X_train, X_test, y_train_pca, y_test_pca = train_test_split(input_df_scaled, principal_components_real, test_size=0.2, random_state=5)\n",
    "\n",
    "X_train_transformed = sm.add_constant(X_train_transformed)\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Models\n",
    "\n",
    "model_1 = sm.OLS(y_train_min_real, X_train).fit()\n",
    "model_2 = sm.OLS(y_train_min_real, X_train_transformed).fit()\n",
    "model_3 = sm.OLS(y_train_pca, X_train).fit()\n",
    "model_4 = sm.OLS(y_train_pca, X_train_transformed).fit()\n",
    "\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Min Real Values + Original Input', 'Min Real Values + PCA Components', 'PCA Real Values + Original Input', 'PCA Real Values + PCA Components'],\n",
    "    'Adjusted R-squared': [model_1.rsquared_adj, model_2.rsquared_adj, model_3.rsquared_adj, model_4.rsquared_adj],\n",
    "    'AIC': [model_1.aic, model_2.aic, model_3.aic, model_4.aic],\n",
    "    'BIC': [model_1.bic, model_2.bic, model_3.bic, model_4.bic],\n",
    "})\n",
    "\n",
    "comparison_df_transposed = comparison_df.T\n",
    "comparison_df_transposed = comparison_df.set_index('Model').T\n",
    "\n",
    "comparison_df_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the table, manipulating the input data doesn’t seem to have a significant impact on the outcomes. However, observing that PCA-applied inputs perform similarly to the original outputs implies that we were able to achieve comparable results using fewer inputs.\n",
    "\n",
    "Also, PCA applied outputs seem to capture more variance compared to the minimum value approach since it has a slightly higher R-squared value and lower errors. This result is expected as PCA is a more sophisticated method that captures the variability in the data more effectively.\n",
    "\n",
    "Now, let's inspect the best model in more detail. The plot below shows the predicted values and the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(y_train_pca, model_3.predict(X_train), c=\"red\")\n",
    "\n",
    "plt.plot(y_train_pca, y_train_pca, color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Actual values\")\n",
    "plt.ylabel(\"Predicted values\")\n",
    "plt.title(\"Predicted vs Actual values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot suggests that model performs well in predicting some values but in general it lacks some kind of information to capture the dynamics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s apply the same models to the imaginary part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "\n",
    "X_train_transformed, X_test_transformed, y_train_min_real, y_test_min_real = train_test_split(transformed_input_df, min_values_img, test_size=0.2, random_state=16)\n",
    "X_train, X_test, y_train_pca, y_test_pca = train_test_split(input_df_scaled, principal_components_img, test_size=0.2, random_state=16)\n",
    "\n",
    "X_train_transformed = sm.add_constant(X_train_transformed)\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# Models\n",
    "\n",
    "model_1 = sm.OLS(y_train_min_real, X_train).fit()\n",
    "model_2 = sm.OLS(y_train_min_real, X_train_transformed).fit()\n",
    "model_3 = sm.OLS(y_train_pca, X_train).fit()\n",
    "model_4 = sm.OLS(y_train_pca, X_train_transformed).fit()\n",
    "\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Min Img. Values + Original Input', 'Min Img. Values + PCA Components', 'PCA Img. Values + Original Input', 'PCA Img. Values + PCA Components'],\n",
    "    'Adjusted R-squared': [model_1.rsquared_adj, model_2.rsquared_adj, model_3.rsquared_adj, model_4.rsquared_adj],\n",
    "    'AIC': [model_1.aic, model_2.aic, model_3.aic, model_4.aic],\n",
    "    'BIC': [model_1.bic, model_2.bic, model_3.bic, model_4.bic],\n",
    "})\n",
    "\n",
    "comparison_df_transposed = comparison_df.T\n",
    "comparison_df_transposed = comparison_df.set_index('Model').T\n",
    "\n",
    "comparison_df_transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, PCA applied outputs shows the effectiveness of the method. It captures more variance compared to the minimum value approach since it has a higher R-squared value and lower errors. This result is expected as discussed before. \n",
    "\n",
    "Now, let's inspect the best model in more detail. The plot below shows the predicted values and the actual values. Plot shows that linear model can capture its distribution at some level but it lacks some kind of information to capture the dynamics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(y_train_pca, model_3.predict(X_train), c=\"red\")\n",
    "\n",
    "plt.plot(y_train_pca, y_train_pca, color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Actual values\")\n",
    "plt.ylabel(\"Predicted values\")\n",
    "plt.title(\"Predicted vs Actual values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, this assignment aimed to investigate the connection between antenna geometry parameters and S11 parameter values. By employing Principal Component Analysis (PCA) on the output data, the number of output variables was reduced to a single principal component that collectively explained some part of the variance in the data. This approach proved advantageous as it simplified the data and enhanced the efficiency of the predictive models. The results of the linear models indicated that the PCA-derived outputs were more effective in capturing the variability in the data compared to the approach that utilized the minimum value. This finding suggests that PCA is a more sophisticated method capable of capturing the dynamics of the data more effectively. Nevertheless, the linear models were unable to fully capture the dynamics of the data, implying that more sophisticated machine learning techniques or more detailed manipulations in the input data may be necessary to enhance the predictive accuracy of the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code <a name=\"code\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click [here](https://github.com/BU-IE-582/fall-24-lmfaraday/blob/main/Homework%201/code.ipynb) to access the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
